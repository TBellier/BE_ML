{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BE Data Analysis - Thomas Bellier and Nathan Magnan\n",
    "### Titanic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time #computation time analysis\n",
    "import os #path handling\n",
    "import numpy as np #import numpy drives sklearn to use numpy arrays instead of python lists\n",
    "import pandas as pd #CSV and dataframe handling\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neighbors import KNeighborsClassifier #kNN classifier\n",
    "from sklearn.svm import LinearSVC #linear classifier\n",
    "from sklearn.neural_network import MLPClassifier #MLP classifier\n",
    "from sklearn.ensemble import AdaBoostClassifier #Ada classifier\n",
    "from sklearn.tree import DecisionTreeClassifier #Decision tree classifier\n",
    "from sklearn.model_selection import train_test_split #Data set splitting functions\n",
    "from sklearn.metrics import confusion_matrix #Confusion matrix\n",
    "#fixing the seed for all random actions\n",
    "np.random.seed(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataPath = './' # path to folder containing the titanic data\n",
    "dataFile = os.path.join(dataPath,'titanic.csv') # data file to use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Data import and formatting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Name</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Ticket</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PassengerId</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>45.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>26.5500</td>\n",
       "      <td>Romaine, Mr. Charles Hallace (\"Mr C Rolmane\")</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>male</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>111428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>34.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>6.4958</td>\n",
       "      <td>Johanson, Mr. Jakob Alfred</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3101264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>831</th>\n",
       "      <td>15.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>C</td>\n",
       "      <td>14.4542</td>\n",
       "      <td>Yasbeck, Mrs. Antoni (Selini Alexander)</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>female</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>906</th>\n",
       "      <td>47.0</td>\n",
       "      <td>E31</td>\n",
       "      <td>S</td>\n",
       "      <td>61.1750</td>\n",
       "      <td>Chaffee, Mrs. Herbert Fuller (Carrie Constance...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>W.E.P. 5734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>854</th>\n",
       "      <td>16.0</td>\n",
       "      <td>D28</td>\n",
       "      <td>S</td>\n",
       "      <td>39.4000</td>\n",
       "      <td>Lines, Miss. Mary Conover</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>PC 17592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>18.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>11.5000</td>\n",
       "      <td>Andrew, Mr. Edgardo Samuel</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>231945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>2.0</td>\n",
       "      <td>C22 C26</td>\n",
       "      <td>S</td>\n",
       "      <td>151.5500</td>\n",
       "      <td>Allison, Miss. Helen Loraine</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>384</th>\n",
       "      <td>35.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>52.0000</td>\n",
       "      <td>Holverson, Mrs. Alexander Oskar (Mary Aline To...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>113789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1070</th>\n",
       "      <td>36.0</td>\n",
       "      <td>F4</td>\n",
       "      <td>S</td>\n",
       "      <td>39.0000</td>\n",
       "      <td>Becker, Mrs. Allen Oliver (Nellie E Baumgardner)</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>female</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>230136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>29.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>9.5000</td>\n",
       "      <td>Sheerlinck, Mr. Jan Baptist</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>345779</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Age    Cabin Embarked      Fare  \\\n",
       "PassengerId                                     \n",
       "188          45.0      NaN        S   26.5500   \n",
       "203          34.0      NaN        S    6.4958   \n",
       "831          15.0      NaN        C   14.4542   \n",
       "906          47.0      E31        S   61.1750   \n",
       "854          16.0      D28        S   39.4000   \n",
       "145          18.0      NaN        S   11.5000   \n",
       "298           2.0  C22 C26        S  151.5500   \n",
       "384          35.0      NaN        S   52.0000   \n",
       "1070         36.0       F4        S   39.0000   \n",
       "82           29.0      NaN        S    9.5000   \n",
       "\n",
       "                                                          Name  Parch  Pclass  \\\n",
       "PassengerId                                                                     \n",
       "188              Romaine, Mr. Charles Hallace (\"Mr C Rolmane\")      0       1   \n",
       "203                                 Johanson, Mr. Jakob Alfred      0       3   \n",
       "831                    Yasbeck, Mrs. Antoni (Selini Alexander)      0       3   \n",
       "906          Chaffee, Mrs. Herbert Fuller (Carrie Constance...      0       1   \n",
       "854                                  Lines, Miss. Mary Conover      1       1   \n",
       "145                                 Andrew, Mr. Edgardo Samuel      0       2   \n",
       "298                               Allison, Miss. Helen Loraine      2       1   \n",
       "384          Holverson, Mrs. Alexander Oskar (Mary Aline To...      0       1   \n",
       "1070          Becker, Mrs. Allen Oliver (Nellie E Baumgardner)      3       2   \n",
       "82                                 Sheerlinck, Mr. Jan Baptist      0       3   \n",
       "\n",
       "                Sex  SibSp  Survived       Ticket  \n",
       "PassengerId                                        \n",
       "188            male      0         1       111428  \n",
       "203            male      0         0      3101264  \n",
       "831          female      1         1         2659  \n",
       "906          female      1         1  W.E.P. 5734  \n",
       "854          female      0         1     PC 17592  \n",
       "145            male      0         0       231945  \n",
       "298          female      1         0       113781  \n",
       "384          female      1         1       113789  \n",
       "1070         female      0         1       230136  \n",
       "82             male      0         1       345779  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we assign column names based on the description file. The column names are included in the csv file.\n",
    "fullDF = pd.read_csv(dataFile, index_col='PassengerId') \n",
    "fullDF.sample(10) # let's take a random sample from the full data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's define a mapping function from 'Embarked' to a class number\n",
    "def get_Embarked_dummies(data):\n",
    "    \"\"\"\n",
    "    Creates dummy columns to separate the three Embark ports\n",
    "    Args:\n",
    "       data: (pandas.dataframe) embarked values\n",
    "    Returns: (pandas.dataframe) embark columns\n",
    "    \"\"\"\n",
    "    data = data['Embarked']\n",
    "    columns = pd.get_dummies(data)\n",
    "    return columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's define a mapping function from 'Pclass' to a class number\n",
    "def get_Pclass_dummies(data):\n",
    "    \"\"\"\n",
    "    Creates dummy columns to separate the three classes\n",
    "    Args:\n",
    "       data: (pandas.dataframe) classes values\n",
    "    Returns: (pandas.dataframe) classes columns\n",
    "    \"\"\"\n",
    "    data = data['Pclass']\n",
    "    new_columns = pd.get_dummies(data)\n",
    "    new_columns = new_columns.rename(columns={1: 'class1', 2: 'class2', 3: 'class3'})\n",
    "    return new_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's define a mapping function from 'Sex' to a class number\n",
    "def sexMap(Sex):\n",
    "  \"\"\"\n",
    "  Sex mapping function. Assigns a sex class number, given a sex name\n",
    "  Args:\n",
    "    Sex: (str) sex\n",
    "  Returns: (int) sex class\n",
    "  \"\"\"\n",
    "\n",
    "  label = np.nan\n",
    "  if Sex == 'female':\n",
    "    label = 0\n",
    "  elif Sex == 'male':\n",
    "    label = 1\n",
    "  else:\n",
    "    raise(RuntimeWarning(f'Unknown sex type: {Sex}, using default label NaN'))\n",
    "  \n",
    "  return label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les paramètres 'Pclass' et 'Embarked' possèdent trois valeurs différentes, qui ne sont pas nécéssairement liées linéairement entre elles (voire indépendantes pour 'Embarked') ; on va donc créer des colonnes binaires, qui correspondent à chacune des valeurs que peuvent prendre ces paramètres. Cela premettra de bien traiter chaque port d'embarquement et chaque classe comme des valeurs indépendantes dans la suite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's apply the mapping functions to the input data\n",
    "fullDF = fullDF.join(get_Embarked_dummies(fullDF))\n",
    "fullDF = fullDF.join(get_Pclass_dummies(fullDF))\n",
    "fullDF['Sex'] = fullDF['Sex'].map(sexMap)\n",
    "usefulDF = fullDF[['Age', 'S', 'Q', 'C', 'Fare', 'Parch', 'class1', 'class2', 'class3', 'Sex', 'SibSp', 'Survived']]\n",
    "usefulDF = usefulDF.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>S</th>\n",
       "      <th>Q</th>\n",
       "      <th>C</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Parch</th>\n",
       "      <th>class1</th>\n",
       "      <th>class2</th>\n",
       "      <th>class3</th>\n",
       "      <th>Sex</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Survived</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PassengerId</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>635</th>\n",
       "      <td>9.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>27.9000</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>756</th>\n",
       "      <td>0.67</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>14.5000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>19.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.8542</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>618</th>\n",
       "      <td>26.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>16.1000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1280</th>\n",
       "      <td>21.00</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.7500</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>598</th>\n",
       "      <td>49.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>863</th>\n",
       "      <td>48.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>25.9292</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309</th>\n",
       "      <td>30.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>24.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>870</th>\n",
       "      <td>4.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11.1333</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>44.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>27.7208</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Age  S  Q  C     Fare  Parch  class1  class2  class3  Sex  \\\n",
       "PassengerId                                                                \n",
       "635           9.00  1  0  0  27.9000      2       0       0       1    0   \n",
       "756           0.67  1  0  0  14.5000      1       0       1       0    1   \n",
       "193          19.00  1  0  0   7.8542      0       0       0       1    0   \n",
       "618          26.00  1  0  0  16.1000      0       0       0       1    0   \n",
       "1280         21.00  0  1  0   7.7500      0       0       0       1    1   \n",
       "598          49.00  1  0  0   0.0000      0       0       0       1    1   \n",
       "863          48.00  1  0  0  25.9292      0       1       0       0    0   \n",
       "309          30.00  0  0  1  24.0000      0       0       1       0    1   \n",
       "870           4.00  1  0  0  11.1333      1       0       0       1    1   \n",
       "195          44.00  0  0  1  27.7208      0       1       0       0    0   \n",
       "\n",
       "             SibSp  Survived  \n",
       "PassengerId                   \n",
       "635              3         0  \n",
       "756              1         1  \n",
       "193              1         1  \n",
       "618              1         0  \n",
       "1280             0         0  \n",
       "598              0         0  \n",
       "863              0         1  \n",
       "309              1         0  \n",
       "870              1         1  \n",
       "195              0         1  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's check that mapping works on a few random samples\n",
    "usefulDF.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate data from classes\n",
    "dataDF = usefulDF[['Age', 'S', 'Q', 'C', 'Fare', 'Parch', 'class1', 'class2', 'class3', 'Sex', 'SibSp']]\n",
    "classDF = usefulDF['Survived']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Data splitting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's split the data into training data, and test data. Same splitting should be applied to classes.\n",
    "# Here, the test data size is 10% of the full dataset\n",
    "trainData,testData,trainY,testY = train_test_split(dataDF,classDF,test_size=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - kNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On a déjà expliqué l'idée de cet algorithme dans le notebook sur les Iris. Notons simplement que l'on s'attends à de moins bon résultats cette fois-ci, car il y a une part d'_aléatoire_ . C'est à dire que deux fleurs de même taille, de même nombre de pétales, de même couleur, etc. sont toujours de la même espèce. Alors parmi deux passagers de même age, de même sexe, et dans la même classe, l'un peut survivre et pas l'autre.\n",
    "\n",
    "On s'attends en particulier à de pauvres résultats pour le classifieur $k$NN, qui n'est généralement pas très performant sur les problèmes avec de l'aléatoire [2]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='kd_tree', n_jobs=-1, n_neighbors=3)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's build a kd Tree out of our training data and use euclidian distance as a metric.\n",
    "# This can be replaced by a custom metric\n",
    "start_time = time.time()\n",
    "kNN = KNeighborsClassifier(n_neighbors=3,algorithm='kd_tree',metric='minkowski',p=2,n_jobs=-1)\n",
    "kNN.fit(trainData,trainY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[507  61]\n",
      " [105 267]]\n"
     ]
    }
   ],
   "source": [
    "# Check the classifier's performance on the training data\n",
    "trainPredictions = kNN.predict(trainData)\n",
    "trainCM = confusion_matrix(y_pred=trainPredictions,y_true=trainY)\n",
    "print(trainCM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Et comme prévu, le classifieur $k$NN a, pour ce problème-ci, un taux d'erreur de 18% sur les points d'entrainement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - kNN classifier performance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[46 14]\n",
      " [16 29]]\n",
      "0.7142857142857143\n",
      "--- 0.2901310920715332 seconds ---\n"
     ]
    }
   ],
   "source": [
    "# Check the classifier's performance on the test data\n",
    "predictions = kNN.predict(testData)\n",
    "testCM = confusion_matrix(y_pred=predictions,y_true=testY)\n",
    "print(testCM)\n",
    "accuracy = float(np.sum([testCM[i][i] for i in range(len(testCM))]))/float(len(testY))\n",
    "print(accuracy)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le taux d'erreur monte même à 29% pour les points d'entrainement. Mais on suspecte que c'est un effet non significatif, car le classifieur $k$NN standard n'est pas supposé être moins bon sur les points d'entrainement que sur les points de test : il n'as subit aucun entrainement.\n",
    "\n",
    "Le taux d'erreur réel (pour ce problème-ci) est donc sans doute quelque part entre 20% et 40%. On pourrait en obtenir une estimation plus fine avec une méthode de _leave-one-out_ : on fait $n$ groupes de points, on entraine le classifieur sur $n-1$ groupes et on le test sur le dernier groupe. On fait ça $n$ fois en laissant un groupe différent de coté à chaque fois, puis on fait le moyenne des taux d'erreur sur les $n$ tests. On a ainsi des statistiques bien meilleures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1b1f70a4648>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# You can also plot the confusion matrix\n",
    "plt.matshow(testCM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 - Linear classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On a déjà expliqué l'idée de cet algorithme dans le notebook sur les Iris."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC(max_iter=10000000.0)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's build a linear classifier out of our training data and use euclidian distance as a metric.\n",
    "# This can be replaced by a custom metric\n",
    "start_time = time.time()\n",
    "classifier = LinearSVC(max_iter=1e7) #very slow, can be reduced to 1e6 to break it before final convergence\n",
    "classifier.fit(trainData,trainY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the classifier's performance on the training data\n",
    "trainPredictions = classifier.predict(trainData)\n",
    "trainCM = confusion_matrix(y_pred=trainPredictions,y_true=trainY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 - Linear classifier performance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[55  5]\n",
      " [12 33]]\n",
      "[[509  59]\n",
      " [ 82 290]]\n",
      "0.8380952380952381\n",
      "--- 112.61645197868347 seconds ---\n"
     ]
    }
   ],
   "source": [
    "# Check the classifier's performance on the test data\n",
    "predictions = classifier.predict(testData)\n",
    "testCM = confusion_matrix(y_pred=predictions,y_true=testY)\n",
    "print(testCM)\n",
    "print(trainCM)\n",
    "accuracy = float(np.sum([testCM[i][i] for i in range(len(testCM))]))/float(len(testY))\n",
    "print(accuracy)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il semblerait que les résultats du classifieur linéaire soit meilleurs que ceux du classifieur $k$NN. Ce n'est pas absurde : le classifieur linéaire a été entrainé pour ce problème précis, dons spécialisé pour cette tâche. Ce n'est pas le cas du classifieur $k$NN. Mais cet argument est vraiment très heuristique, il est facile de construire un jeu de données où le classifieur $k$NN est bien meillleur que le classifieur linéaire. <br>\n",
    "Par exemple, on prend une classe dans un disque, et une seconde classe qui l'entoure. La séparation des classes par un hyperplan est impossible, alors que l'on a bien 2 groupes géométriques.\n",
    "\n",
    "Enfin, le classifieur linéaire est bien plus lent, et c'est généralement le cas : il faut un entrainement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1b1f718bd88>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPoAAAECCAYAAADXWsr9AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAFzElEQVR4nO3bMW9dhRnG8fetnVSiG5ClYBUGhJSpg8VXaDKxkhkpEx+AtR+CJVIjNlBHBqQMXViiFk8VqAIipAqLgRS2Vk0a9HZhiCCSr805PsHP77fdo6PjRzr+61zL9/bMFHCx/WrrAcD6hA4BhA4BhA4BhA4BhA4BhH4K3X2tuz/r7nvd/fbWe9hdd9/u7m+6+5Ott2xB6Dvq7r2qeqeqrlfV1aq60d1Xt13FKbxbVde2HrEVoe/utaq6NzNfzszDqnq/ql7feBM7mpmPquq7rXdsRei7e6Gqvnrs9fEPx+CpJ/Td9ROO+fwwvwhC391xVR089vrFqvp6oy1wKkLf3cdV9Up3v9zdl6vqjar6YONNsBOh72hmHlXVW1V1p6r+UVV/nplPt13Frrr7vaq6W1Wvdvdxd7+59abz1L6mChefJzoEEDoEEDoEEDoEEDoEEPopdffNrTdwdqn3T+inF/mLcoFE3j+hQ4BVPjDz/LN789LBpcWv+zS4/+33deW5va1nrOrzvz+z9YTV/K8e1KX69dYzVvPf+nc9nAc/+QLW/ho/7KWDS/W3Owcnn8hT6Q+//f3WEzijv85fnnjcW3cIIHQIIHQIIHQIIHQIIHQIIHQIIHQIIHQIIHQIIHQIIHQIIHQIIHQIIHQIIHQIIHQIIHQIIHQIIHQIIHQIIHQIIHQIIHQIIHQIIHQIIHQIIHQIIHQIIHQIIHQIIHQIIHQIIHQIIHQIIHQIIHQIIHQIIHQIIHQIIHQIIHQIIHQIIHQIIHQIIHQIIHQIIHQIIHQIIHQIsFPo3X2tuz/r7nvd/fbao4BlnRh6d+9V1TtVdb2qrlbVje6+uvYwYDm7PNFfq6p7M/PlzDysqver6vV1ZwFL2iX0F6rqq8deH/9wDPiF2CX0fsKx+clJ3Te7+6i7j+5/+/3PXwYsZpfQj6vq4LHXL1bV1z8+aWZuzczhzBxeeW5vqX3AAnYJ/eOqeqW7X+7uy1X1RlV9sO4sYEn7J50wM4+6+62qulNVe1V1e2Y+XX0ZsJgTQ6+qmpkPq+rDlbcAK/HJOAggdAggdAggdAggdAggdAggdAggdAggdAggdAggdAggdAggdAggdAggdAggdAggdAggdAggdAggdAggdAggdAggdAggdAggdAggdAggdAggdAggdAggdAggdAggdAggdAggdAggdAggdAggdAggdAggdAggdAggdAggdAggdAggdAggdAggdAggdAggdAggdAggdAiwv8ZFv/ji2bp+/cYal+YcfP6n32w9gTN68Me7TzzuiQ4BhA4BhA4BhA4BhA4BhA4BhA4BhA4BhA4BhA4BhA4BhA4BhA4BhA4BhA4BhA4BhA4BhA4BhA4BhA4BhA4BhA4BhA4BhA4BhA4BhA4BhA4BhA4BhA4BhA4BhA4BhA4BhA4BhA4BhA4BhA4BhA4BhA4BhA4BhA4BhA4BhA4BhA4BhA4BhA4BhA4BhA4BhA4BhA4BhA4BhA4BhA4BTgy9u2939zfd/cl5DAKWt8sT/d2qurbyDmBFJ4Y+Mx9V1XfnsAVYib/RIcBioXf3ze4+6u6jh4/+s9RlgQUsFvrM3JqZw5k5vLz/zFKXBRbgrTsE2OXfa+9V1d2qerW7j7v7zfVnAUvaP+mEmblxHkOA9XjrDgGEDgGEDgGEDgGEDgGEDgGEDgGEDgGEDgGEDgGEDgGEDgGEDgGEDgGEDgGEDgGEDgGEDgGEDgGEDgGEDgGEDgGEDgGEDgGEDgGEDgGEDgGEDgGEDgGEDgGEDgGEDgGEDgGEDgGEDgGEDgGEDgGEDgGEDgGEDgGEDgGEDgGEDgGEDgGEDgGEDgGEDgGEDgGEDgGEDgF6Zpa/aPf9qvrn4hd+OjxfVf/aegRndtHv3+9m5sqPD64S+kXW3Uczc7j1Ds4m9f556w4BhA4BhH56t7YewM8Sef/8jQ4BPNEhgNAhgNAhgNAhgNAhwP8B0q635wgSUtAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# You can also plot the confusion matrix\n",
    "plt.matshow(testCM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7 - MLP classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On a déjà expliqué l'idée de cet algorithme dans le notebook sur les Iris."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='logistic', max_iter=10000)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's build a MLP classifier.\n",
    "start_time = time.time()\n",
    "mlp = MLPClassifier(solver='adam', activation='logistic', max_iter=10000)\n",
    "mlp.fit(trainData,trainY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[517  51]\n",
      " [ 77 295]]\n"
     ]
    }
   ],
   "source": [
    "# Check the classifier's performance on the training data\n",
    "trainPredictions = mlp.predict(trainData)\n",
    "trainCM = confusion_matrix(y_pred=trainPredictions,y_true=trainY)\n",
    "print(trainCM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8 - MLP classifier performance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[56  4]\n",
      " [11 34]]\n",
      "0.8571428571428571\n",
      "--- 0.7802231311798096 seconds ---\n"
     ]
    }
   ],
   "source": [
    "# Check the classifier's performance on the test data\n",
    "predictions = mlp.predict(testData)\n",
    "testCM = confusion_matrix(y_pred=predictions,y_true=testY)\n",
    "print(testCM)\n",
    "accuracy = float(np.sum([testCM[i][i] for i in range(len(testCM))]))/float(len(testY))\n",
    "print(accuracy)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le classifieur MLP atteint des performances simillaires au classifieur linéaire, mais dans des temps de calcul plus court.\n",
    "\n",
    "Nous ne sommes pas surpris : le MLP est adapté aux problèmes avec une part d'aléatoire, car il est construit dés l'origine dans un cadre Bayésien. Il est également beaucoup plus rapide grâce à sa structure en couches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1b1f71c4408>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# You can also plot the confusion matrix\n",
    "plt.matshow(testCM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9 - Decision tree classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme c'est la première fois qu'on l'utilise, on rapelle l'idée générale du classifieur par arbre de décision. C'est un classifieur assez abstrait, mais très rapide à entrainer et à appeler [5].\n",
    "\n",
    "L'idée est de déterminer quelle est la feature qui contient le plus d'information, et de faire une décision booléenne selon la valeur de cette feature, pour séparer autant que possible les classes. On divise en 2 groupes nos points d'entrainements selon cette première règle de décision, puis on appelle l'algoritme sur chacun des 2 nouveaux training sets. <br>\n",
    "Après $n$ étapes récursives de ce type, on obtient un arbre de décision de profondeur $n$, où chaque noeud contient une règle de décision très simple portant sur la valeur d'une feature, et chaque feuille indique la classification associée à ce chemin de longeur $n$ dans l'arbre [1].\n",
    "\n",
    "L'algorithme que l'on vient de décrire est évidemment glouton. Idéalement, les règles de décisions seraient choisies pour atteindre un optima global en pouvoir de prédiction. Mais on montre que ce problème est NP-complet [4]. Ainsi, l'algorithme que l'on a décrit n'est pas si loin de celui qui est implémenté en réalité.\n",
    "\n",
    "En revanche, ce classifieur a plusieurs défauts. D'abord, il est très susceptible à l'overfitting, surtout si l'utilisateur demande un arbre avec trop d'étages [5]. Aussi, l'arbre est instable dans le sens où une petite variation des points d'entrainement peut changer du tout au tout les règles de décision choisies durant l'entrainement [5]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier()"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's build a decision tree classifier.\n",
    "start_time = time.time()\n",
    "dtc = DecisionTreeClassifier()\n",
    "dtc.fit(trainData,trainY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[568   0]\n",
      " [ 11 361]]\n"
     ]
    }
   ],
   "source": [
    "# Check the classifier's performance on the training data\n",
    "trainPredictions = dtc.predict(trainData)\n",
    "trainCM = confusion_matrix(y_pred=trainPredictions,y_true=trainY)\n",
    "print(trainCM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On voit que les prédiction du classifieur à arbre de décision, sur ses points d'entrainement, sont très bonnes par arpport aux autres classifieurs. Cela confirme que ce classifieur se spécialise énormément pour expliquer les points d'entrainement que l'on lui fournit. Mais peut-être qu'il se spécialise trop ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10 - Decision tree classifier performance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[51  9]\n",
      " [12 33]]\n",
      "0.8\n",
      "--- 0.03517889976501465 seconds ---\n"
     ]
    }
   ],
   "source": [
    "# Check the classifier's performance on the test data\n",
    "predictions = dtc.predict(testData)\n",
    "testCM = confusion_matrix(y_pred=predictions,y_true=testY)\n",
    "print(testCM)\n",
    "accuracy = float(np.sum([testCM[i][i] for i in range(len(testCM))]))/float(len(testY))\n",
    "print(accuracy)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On voit que les résultats sur les points de test sont simmillaires à ceux des autres algorithmes, mais moins bon que les résultats sur les points d'entrainement. Ainsi (si l'on admet une valeur statistique à ces 105 tests), il y a un petit overfitting du classifieur à arbre de décision, mais qui n'impacte pas gravement les performances. Il faut simplement se rappeler à l'avenir de ne pas évaluer la performance de ce classifieur sur ses données d'entrainement !\n",
    "\n",
    "Par contre, ce classifieur est très rapide par rapport aux précédent !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1b1f78b0148>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# You can also plot the confusion matrix\n",
    "plt.matshow(testCM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11 - AdaBoost classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme c'est la première fois qu'on l'utilise, on rapelle l'idée générale du classifieur AdaBoost. C'est un algorithme d'_ensemble learning_ , c'est à dire qu'il appelle plusieurs petits classifieurs qui font chacun une prédiction, puis AdaBoost fait une moyenne pondérée de ces prédictions pour former sa décision [1].\n",
    "\n",
    "En l'occurence, Adaboost entraine un premier classifieur $h_{1}$ sur toutes les données d'entrainement, puis lui faire faire des prédictions. Il note les points qui sont mal prédis, puis entraine un second classifieur $h_{2}$ spécifiquement sur ces points. Le classifieur courant devient alors $H_{2} = h_{1} + \\alpha_{2} h_{2}$, avec $\\alpha_{2}$ optimisé pour minimiser le taux d'erreur du classifieur courant. On recommence alors le processus : on regarde où est-ce-que $H_{2}$ se trompe, on entraine un classifieur $h_{3}$ sur ces points, puis le classifieur courant devient $H_{3} = h_{1} + \\alpha_{2} h_{2} + \\alpha_{3} h_{3}$. Et ainsi de suite sur $n$ étapes [1].\n",
    "\n",
    "C'est un algorithme très performant, en particulier sur les problème à haute dimensionalité car l'algorithme sélectionne les features les plus intéressantes [6]. C'est aussi un algorithme un peu moins sensible à l'overfitting que les arbres de décision. En fait, c'est considéré comme le meilleur classifieur généraliste [6]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=1))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's build a AdaBoost classifier.\n",
    "start_time = time.time()\n",
    "ada = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=1))\n",
    "ada.fit(trainData,trainY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[516  52]\n",
      " [ 68 304]]\n"
     ]
    }
   ],
   "source": [
    "# Check the classifier's performance on the training data\n",
    "trainPredictions = ada.predict(trainData)\n",
    "trainCM = confusion_matrix(y_pred=trainPredictions,y_true=trainY)\n",
    "print(trainCM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les prédiction sur les points d'entrainement est moins bonne que pour le classifieur par arbre de décision, mais sur un pied d'egalité par rapport aux autres algorithmes. Comme on a déjà des taux d'erreurs assez haut sur les points d'entrainement, on peut penser qu'il n'y as pas eu d'overfitting. \n",
    "\n",
    "Et comme c'est le quatrième algorithme indépendant à fournir ce niveau de performance sur les points d'entrainement, on peut imaginer que 12% est l'ordre de grandeur du bruit dans les données Titanic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12 - AdaBoost classifier performance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[55  5]\n",
      " [11 34]]\n",
      "0.8476190476190476\n",
      "--- 0.10648107528686523 seconds ---\n"
     ]
    }
   ],
   "source": [
    "# Check the classifier's performance on the test data\n",
    "predictions = ada.predict(testData)\n",
    "testCM = confusion_matrix(y_pred=predictions,y_true=testY)\n",
    "print(testCM)\n",
    "accuracy = float(np.sum([testCM[i][i] for i in range(len(testCM))]))/float(len(testY))\n",
    "print(accuracy)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les prédictions sont aussi bonne, voire legerement meilleurs que pour l'arbre de décision. Ce qui confirme qu'il y a eu un petit overfitting pour l'algorithme précédent. Et aussi qu'Adaboost est légerement moins sensible à l'overfitting.\n",
    "\n",
    "En terme de vitesse, Adaboost est plus lent que l'arbre de décision, ce qui est cohérent car il en utilise plusieurs, mais il reste plus rapide que les classifieurs les plus anciens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1b1f7930848>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# You can also plot the confusion matrix\n",
    "plt.matshow(testCM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Références\n",
    "\n",
    "[1] Notes de cours\n",
    "\n",
    "[2] Page Wikiepdia sur le classifieur $k$NN : https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm#Properties\n",
    "\n",
    "[3] Page Wikiepdia sur le classifieur MLP : https://en.wikipedia.org/wiki/Multilayer_perceptron\n",
    "\n",
    "[4] Page Wikipedia sur les arbres de décision : https://en.wikipedia.org/wiki/Decision_tree_learning\n",
    "\n",
    "[5] Documentation de scikit-learn sur les arbres de décision : https://scikit-learn.org/stable/modules/tree.html\n",
    "\n",
    "[6] Page Wikipedia sur AdaBoost : https://en.wikipedia.org/wiki/AdaBoost#Overview"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
